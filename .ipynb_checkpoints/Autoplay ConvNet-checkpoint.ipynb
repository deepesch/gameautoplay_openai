{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"tutorial.gym.atari.spaceinvaders-v0.cnn\"\n",
    "snapshot_path = \".\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'AirRaid-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 10\n",
    "\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=gpu2'\n",
      "mkdir: /Users/deepesch/agentnet_snapshots/: File exists\r\n"
     ]
    }
   ],
   "source": [
    "#this is my machine-specific config. replace if are not me.\n",
    "\n",
    "#theano device selection\n",
    "%env THEANO_FLAGS='device=gpu2'\n",
    "\n",
    "\n",
    "#snapshot path - where neural network snapshots are saved during the main training loop\n",
    "!mkdir ~/agentnet_snapshots/\n",
    "snapshot_path = \"~/agentnet_snapshots/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose your side\n",
    "\n",
    "#Sith\n",
    "mode = \"simple convnet\"\n",
    "\n",
    "#Jedi\n",
    "#mode = \"dense nn\"\n",
    "\n",
    "#Stormtrooper\n",
    "#mode = \"linear regression\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepesch/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-05-03 14:13:07,539] Making new env: AirRaid-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1100987d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL4AAAEACAYAAAANw8wsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTrLgGyf9P2bZy2WuLCs3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fc77790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print action_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "  \n",
    "### Simple no-memory model\n",
    "\n",
    "Since we have almost fully observable environment AND we want to keep baseline simple, we shall use NO agent memories and a simple lasagne CNN to process observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.memory import WindowAugmentation\n",
    "from agentnet.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using some CNNs\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "\n",
    "#image observation\n",
    "observation_layer = lasagne.layers.InputLayer(observation_shape,\n",
    "                                                    name=\"images input\")\n",
    "\n",
    "observation_reshape = lasagne.layers.dimshuffle(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n",
    "\n",
    "#convolution\n",
    "if mode == 'simple convnet':\n",
    "    print \"Using some CNNs\"\n",
    "    cnn = lasagne.layers.Conv2DLayer(observation_reshape,num_filters=32,filter_size=(5,5),name='cnn0')\n",
    "    cnn = lasagne.layers.MaxPool2DLayer(cnn,(5,5), name='pool0')\n",
    "    cnn = lasagne.layers.Conv2DLayer(cnn,num_filters=64,filter_size=(5,5),name='cnn1')\n",
    "    cnn = lasagne.layers.MaxPool2DLayer(cnn,(5,5), name='pool1')\n",
    "    dnn = lasagne.layers.DropoutLayer(cnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "    dnn = lasagne.layers.DenseLayer(dnn,num_units=500,name='dense1')\n",
    "    nn = dnn\n",
    "#dense with dropout\n",
    "elif mode == \"dense nn\":\n",
    "    print \"Using simple dense network\"\n",
    "    \n",
    "    dnn = lasagne.layers.DenseLayer(observation_reshape,num_units=300,name='dense0')\n",
    "    dnn = lasagne.layers.DropoutLayer(dnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "    dnn = lasagne.layers.DenseLayer(dnn,num_units=100,name='dense1')\n",
    "    nn = dnn\n",
    "\n",
    "#linear regression\n",
    "else:\n",
    "    print \"Using linear model\"\n",
    "    nn = observation_reshape\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "flat_nn = lasagne.layers.FlattenLayer(nn,outdim=2,name='flat frame output')\n",
    "\n",
    "\n",
    "#memory\n",
    "#using simple window-based memory that stores sevelar states\n",
    "#the environment does not need any more\n",
    "\n",
    "window_size = 3\n",
    "prev_window = lasagne.layers.InputLayer((None,window_size,flat_nn.output_shape[1]),\n",
    "                                        name = \"previous window state\")\n",
    "\n",
    "window = WindowAugmentation(flat_nn,prev_window,name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "memory_dict = {window:prev_window}\n",
    "\n",
    "\n",
    "#q_eval\n",
    "q_eval = lasagne.layers.DenseLayer(window,\n",
    "                                   num_units = n_actions,\n",
    "                                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                                   name=\"QEvaluator\")\n",
    "#resolver\n",
    "epsilon = theano.shared(np.float32(0.5),\"e-greedy.epsilon\")\n",
    "\n",
    "resolver = EpsilonGreedyResolver(q_eval,epsilon=epsilon,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cnn0.W,\n",
       " cnn0.b,\n",
       " cnn1.W,\n",
       " cnn1.b,\n",
       " dense1.W,\n",
       " dense1.b,\n",
       " QEvaluator.W,\n",
       " QEvaluator.b]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * to be covered where they are more useful\n",
    " \n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* compute action and next state given observation and prev state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "applier_observation = T.tensor4(\"input image\",dtype=floatX)\n",
    "\n",
    "applier_window = T.tensor3(\"prev window\",dtype=floatX)\n",
    "\n",
    "\n",
    "res =agent.get_agent_reaction({window:applier_window},\n",
    "                              applier_observation,\n",
    "                              deterministic = True #disable dropout here. Only enable in experience replay\n",
    "                             )\n",
    "\n",
    "\n",
    "applier_actions,applier_new_states,applier_policy = res\n",
    "\n",
    "applier_fun = theano.function([applier_observation,applier_window],\n",
    "        applier_actions+applier_new_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a nice pythonic interface\n",
    "def step(observation, prev_memories = 'zeros'):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((N_PARALLEL_GAMES,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype=floatX) \n",
    "                         for mem in agent.state_variables]\n",
    "    \n",
    "    res = applier_fun(np.array(observation),prev_memories[0])\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action,memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A whole lot of space invaders\n",
    "ataries = [gym.make(GAME_TITLE) for i in range(N_PARALLEL_GAMES)]\n",
    "for atari in ataries:\n",
    "    atari.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interact(ataries,n_steps = 100,verbose=False):\n",
    "    \"\"\"generate interaction sessions with ataries (openAI gym atari environments)\n",
    "    Sessions will have length n_steps. \n",
    "    Each time one of games is finished, it is immediately getting reset\"\"\"\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = map(lambda atari: atari.render('rgb_array'), ataries)\n",
    "    \n",
    "    prev_memory_states = 'zeros'\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        actions,new_memory_states = step(prev_observations,prev_memory_states)\n",
    "\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = zip(*map(lambda atari, action: atari.step(action), \n",
    "                                                            ataries,actions))\n",
    "        \n",
    "        for i in range(len(ataries)):\n",
    "            if is_done[i]:\n",
    "                ataries[i].reset()\n",
    "                if verbose:\n",
    "                    print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,new_memory_states,is_done,infos))\n",
    "        \n",
    "        prev_observations = new_observations\n",
    "        prev_memory_states = new_memory_states\n",
    "                \n",
    "    \n",
    "    \n",
    "    return zip(*history_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "obsevation_log,action_log,reward_log,_,_,_  = interact(ataries,50)\n",
    "\n",
    "\n",
    "print np.array(reward_log)[:10].T\n",
    "print np.array(action_names)[np.array(action_log)[:3,:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "### we shall use session pool environment to immediately train on new sessions\n",
    "* theoretically, this environemnt is designed for storing a lot of game sessions and training on random batches,\n",
    "* but for the sake of baseline, it's a one-time usage pool\n",
    "\n",
    "1. Interact with ALE, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_pool(env, ataries,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    \n",
    "    obsevation_log,action_log,reward_log,_,is_done_log,_= interact(ataries,n_steps=n_steps)\n",
    "    \n",
    "    \n",
    "    #tensor dimensions\n",
    "    \n",
    "    # [batch_i, time_i, width, height, rgb]\n",
    "    observation_tensor = np.array(obsevation_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i,time_i]\n",
    "    action_tensor = np.array(action_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i, time_i]\n",
    "    reward_tensor = np.array(reward_log).swapaxes(0,1)\n",
    "\n",
    "    # [batch_i, time_i]\n",
    "    is_alive_tensor = 1- np.array(is_done_log,dtype = 'int8').swapaxes(0,1)\n",
    "    \n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,[])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,ataries,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#produce interaction sequences of length <= 10\n",
    "\n",
    "_,observation_seq,_,_,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "#observation seq are the observation tensor we just loaded\n",
    "#qvalues seq are agent's Qvalues obtained via experience replay\n",
    "\n",
    "\n",
    "#The three \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#second - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n",
    "\n",
    "#the actions agent took in the original recorded game\n",
    "action_seq = env.actions[0]\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.rewards\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.is_alive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.95),name = 'q_learning_gamma')\n",
    "\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                        action_seq,\n",
    "                                                        rewards_seq,\n",
    "                                                        is_alive_seq,\n",
    "                                                        gamma_or_gammas=gamma,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "mse_loss = elwise_mse_loss.sum() / is_alive_seq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = lasagne.updates.adadelta(loss,\n",
    "                                             weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,mse_loss,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session visualization tools\n",
    "\n",
    "### [warning, this thing basicly tries to track various Qvalues over time]\n",
    "### [but it's bulky and stupid, so don't try to understand it if it didn't come naturally]\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* usually useful to get insight on what worked and what din't\n",
    "* in this case, we print strings following pattern\n",
    "  * [action_name] ([predicted action qvalue]) -> reward [reference qvalue] | next iteration\n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are Qvalues for actions\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "__Warning! the visualization tools are underdeveloped and only allow simple operations.__\n",
    "\n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        qvalues_seq, action_seq,rewards_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False,max_n_sessions = 3,update = True):\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in ataries[:max_n_sessions]]\n",
    "    \n",
    "    if update:\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "    \n",
    "    \n",
    "    printables = get_printables()\n",
    "    \n",
    "    \n",
    "    for i in range(max_n_sessions):\n",
    "        plt.imshow(pictures[i])\n",
    "        plt.show()\n",
    "            \n",
    "        qvalues_log,actions_log,reward_log, is_alive_log = map(lambda v: np.array(v[i:i+1]), printables)\n",
    "        \n",
    "\n",
    "        print_sessions(qvalues_log,actions_log,reward_log,\n",
    "                       is_alive_seq = is_alive_log,\n",
    "                       action_names=action_names,\n",
    "                       legend = True, #do not show legend since there's too many labeled objects\n",
    "                      plot_policy = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "epsilon.set_value(0.05)\n",
    "display_sessions(with_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tools for model persistence (in progress now. Requires unique names)\n",
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 25000\n",
    "batch_size= 10\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,ataries,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/500.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%50 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "\n",
    "        print \"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy)\n",
    "        print \"rec %.3f reg %.3f\"%(q_loss,l2_penalty)\n",
    "\n",
    "    if epoch_counter %1000 ==0:\n",
    "        print \"Learning curves:\"\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print \"Random session examples\"\n",
    "        display_sessions(with_plots=False)\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %5000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print \"saved\", snap_name\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random session examples\"\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(ataries,n_steps = 100):\n",
    "\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = map(lambda atari: atari.render('rgb_array'), ataries)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        #show pics\n",
    "        map(lambda atari: atari.render('human'), ataries)\n",
    "\n",
    "        \n",
    "        actions = applier_fun(prev_observations)[0]\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = zip(*map(lambda atari, action: atari.step(action), \n",
    "                                                            ataries,actions))\n",
    "        \n",
    "        for i in range(len(ataries)):\n",
    "            if is_done[i]:\n",
    "                ataries[i].reset()\n",
    "                print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,is_done))\n",
    "        prev_observations = new_observations\n",
    "                \n",
    "    \n",
    "    \n",
    "    return zip(*history_log)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "show(ataries[:1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
