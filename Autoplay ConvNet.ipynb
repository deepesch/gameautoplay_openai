{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"tutorial.gym.atari.spaceinvaders-v0.cnn\"\n",
    "snapshot_path = \".\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'AirRaid-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 10\n",
    "\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=gpu2'\n",
      "mkdir: /Users/deepesch/agentnet_snapshots/: File exists\r\n"
     ]
    }
   ],
   "source": [
    "#this is my machine-specific config. replace if are not me.\n",
    "\n",
    "#theano device selection\n",
    "%env THEANO_FLAGS='device=gpu2'\n",
    "\n",
    "\n",
    "#snapshot path - where neural network snapshots are saved during the main training loop\n",
    "!mkdir ~/agentnet_snapshots/\n",
    "snapshot_path = \"~/agentnet_snapshots/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use AgentNet for OpenAI Gym environments\n",
    "\n",
    "* We take Space Invadets game as an example\n",
    "* We train a simple-stupid convolutional network for Q_learning objective\n",
    "* We do have a GPU for that, so if you don't, we have a \"linear regression\" mode - just uncomment it :)\n",
    "* We train via experience replay using SessionPoolEnvironment, explained below\n",
    "* We do NOT use recurrent layers for simplicity\n",
    "* We do NOT use \"smart\" experience replay for simplicity\n",
    "* We use simple stupid one-step q-learning.. you guessed it, for simplicity\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose your side\n",
    "\n",
    "#Sith\n",
    "mode = \"simple convnet\"\n",
    "\n",
    "#Jedi\n",
    "#mode = \"dense nn\"\n",
    "\n",
    "#Stormtrooper\n",
    "#mode = \"linear regression\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lasagne\n",
      "  Using cached Lasagne-0.1.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from lasagne)\n",
      "Building wheels for collected packages: lasagne\n",
      "  Running setup.py bdist_wheel for lasagne ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/deepesch/Library/Caches/pip/wheels/d5/3f/25/2bf9c38906847df05e3529e5df230e349ac5a44b953f1f233b\n",
      "Successfully built lasagne\n",
      "Installing collected packages: lasagne\n",
      "Successfully installed lasagne-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gym[atari] in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages\n",
      "Requirement already up-to-date: numpy>=1.10.4 in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from gym[atari])\n",
      "Requirement already up-to-date: requests>=2.0 in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from gym[atari])\n",
      "Requirement already up-to-date: six in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from gym[atari])\n",
      "Requirement already up-to-date: pyglet in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from gym[atari])\n",
      "Requirement already up-to-date: PyOpenGL in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from gym[atari])\n",
      "Collecting atari-py>=0.0.17 (from gym[atari])\n",
      "  Using cached atari-py-0.0.17.tar.gz\n",
      "Requirement already up-to-date: Pillow in /Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages (from gym[atari])\n",
      "Building wheels for collected packages: atari-py\n",
      "  Running setup.py bdist_wheel for atari-py ... \u001b[?25l-\b \berror\n",
      "  Complete output from command /Users/deepesch/anaconda/envs/pgmpy-env/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" bdist_wheel -d /var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/tmpis225g2upip-wheel- --python-tag cp34:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  mkdir -p build && cd build && cmake .. && make -j4\n",
      "  /bin/sh: cmake: command not found\n",
      "  make: *** [build] Error 127\n",
      "  Could not build atari-py: Command '['make', 'build', '-C', 'atari_py/ale_interface']' returned non-zero exit status 2. (HINT: are you sure cmake is installed? You might also be missing a library. Atari-py requires: zlib [installable as 'apt-get install zlib1g-dev' on Ubuntu].)\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py\", line 33, in <module>\n",
      "      tests_require=['nose2']\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/dist.py\", line 955, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages/wheel/bdist_wheel.py\", line 179, in run\n",
      "      self.run_command('build')\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py\", line 13, in run\n",
      "      subprocess.check_call(['make', 'build', '-C', 'atari_py/ale_interface'])\n",
      "    File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/subprocess.py\", line 561, in check_call\n",
      "      raise CalledProcessError(retcode, cmd)\n",
      "  subprocess.CalledProcessError: Command '['make', 'build', '-C', 'atari_py/ale_interface']' returned non-zero exit status 2\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for atari-py\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for atari-py\n",
      "Failed to build atari-py\n",
      "Installing collected packages: atari-py\n",
      "  Running setup.py install for atari-py ... \u001b[?25l-\b \berror\n",
      "    Complete output from command /Users/deepesch/anaconda/envs/pgmpy-env/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-drikyy73-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    mkdir -p build && cd build && cmake .. && make -j4\n",
      "    /bin/sh: cmake: command not found\n",
      "    make: *** [build] Error 127\n",
      "    Could not build atari-py: Command '['make', 'build', '-C', 'atari_py/ale_interface']' returned non-zero exit status 2. (HINT: are you sure cmake is installed? You might also be missing a library. Atari-py requires: zlib [installable as 'apt-get install zlib1g-dev' on Ubuntu].)\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py\", line 33, in <module>\n",
      "        tests_require=['nose2']\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/dist.py\", line 955, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/dist.py\", line 974, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages/setuptools-20.3-py3.4.egg/setuptools/command/install.py\", line 61, in run\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/command/install.py\", line 539, in run\n",
      "        self.run_command('build')\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/distutils/dist.py\", line 974, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py\", line 13, in run\n",
      "        subprocess.check_call(['make', 'build', '-C', 'atari_py/ale_interface'])\n",
      "      File \"/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/subprocess.py\", line 561, in check_call\n",
      "        raise CalledProcessError(retcode, cmd)\n",
      "    subprocess.CalledProcessError: Command '['make', 'build', '-C', 'atari_py/ale_interface']' returned non-zero exit status 2\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/Users/deepesch/anaconda/envs/pgmpy-env/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-drikyy73-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/p3/m_cvvvc14wl3r8m8_2td3bpr0000gn/T/pip-build-bqpync1_/atari-py/\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f98cd1d00ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0matari\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAME_TITLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages/gym/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMujocoDependencyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You have `requests` version {} installed, but gym requires at least 2.0. HINT: If you directly cloned the GitHub repo, please run `pip install -r requirements.txt` first.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msanity_check_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deepesch/anaconda/envs/pgmpy-env/lib/python3.4/site-packages/gym/__init__.py\u001b[0m in \u001b[0;36msanity_check_dependencies\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrictVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrictVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.10.4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You have `numpy` version {} installed, but gym requires at least 1.10.4. HINT: If you directly cloned the GitHub repo, please run `pip install -r requirements.txt` first.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrictVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrictVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print action_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "  \n",
    "### Simple no-memory model\n",
    "\n",
    "Since we have almost fully observable environment AND we want to keep baseline simple, we shall use NO agent memories and a simple lasagne CNN to process observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.memory import WindowAugmentation\n",
    "from agentnet.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "#image observation\n",
    "observation_layer = lasagne.layers.InputLayer(observation_shape,\n",
    "                                                    name=\"images input\")\n",
    "\n",
    "observation_reshape = lasagne.layers.dimshuffle(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n",
    "\n",
    "#convolution\n",
    "if mode == 'simple convnet':\n",
    "    print \"Using some CNNs\"\n",
    "    cnn = lasagne.layers.Conv2DLayer(observation_reshape,num_filters=32,filter_size=(5,5),name='cnn0')\n",
    "    cnn = lasagne.layers.MaxPool2DLayer(cnn,(5,5), name='pool0')\n",
    "    cnn = lasagne.layers.Conv2DLayer(cnn,num_filters=64,filter_size=(5,5),name='cnn1')\n",
    "    cnn = lasagne.layers.MaxPool2DLayer(cnn,(5,5), name='pool1')\n",
    "    dnn = lasagne.layers.DropoutLayer(cnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "    dnn = lasagne.layers.DenseLayer(dnn,num_units=500,name='dense1')\n",
    "    nn = dnn\n",
    "#dense with dropout\n",
    "elif mode == \"dense nn\":\n",
    "    print \"Using simple dense network\"\n",
    "    \n",
    "    dnn = lasagne.layers.DenseLayer(observation_reshape,num_units=300,name='dense0')\n",
    "    dnn = lasagne.layers.DropoutLayer(dnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "    dnn = lasagne.layers.DenseLayer(dnn,num_units=100,name='dense1')\n",
    "    nn = dnn\n",
    "\n",
    "#linear regression\n",
    "else:\n",
    "    print \"Using linear model\"\n",
    "    nn = observation_reshape\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "flat_nn = lasagne.layers.FlattenLayer(nn,outdim=2,name='flat frame output')\n",
    "\n",
    "\n",
    "#memory\n",
    "#using simple window-based memory that stores sevelar states\n",
    "#the environment does not need any more\n",
    "\n",
    "window_size = 3\n",
    "prev_window = lasagne.layers.InputLayer((None,window_size,flat_nn.output_shape[1]),\n",
    "                                        name = \"previous window state\")\n",
    "\n",
    "window = WindowAugmentation(flat_nn,prev_window,name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "memory_dict = {window:prev_window}\n",
    "\n",
    "\n",
    "#q_eval\n",
    "q_eval = lasagne.layers.DenseLayer(window,\n",
    "                                   num_units = n_actions,\n",
    "                                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                                   name=\"QEvaluator\")\n",
    "#resolver\n",
    "epsilon = theano.shared(np.float32(0.5),\"e-greedy.epsilon\")\n",
    "\n",
    "resolver = EpsilonGreedyResolver(q_eval,epsilon=epsilon,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * to be covered where they are more useful\n",
    " \n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* compute action and next state given observation and prev state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "applier_observation = T.tensor4(\"input image\",dtype=floatX)\n",
    "\n",
    "applier_window = T.tensor3(\"prev window\",dtype=floatX)\n",
    "\n",
    "\n",
    "res =agent.get_agent_reaction({window:applier_window},\n",
    "                              applier_observation,\n",
    "                              deterministic = True #disable dropout here. Only enable in experience replay\n",
    "                             )\n",
    "\n",
    "\n",
    "applier_actions,applier_new_states,applier_policy = res\n",
    "\n",
    "applier_fun = theano.function([applier_observation,applier_window],\n",
    "        applier_actions+applier_new_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a nice pythonic interface\n",
    "def step(observation, prev_memories = 'zeros'):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((N_PARALLEL_GAMES,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype=floatX) \n",
    "                         for mem in agent.state_variables]\n",
    "    \n",
    "    res = applier_fun(np.array(observation),prev_memories[0])\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action,memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A whole lot of space invaders\n",
    "ataries = [gym.make(GAME_TITLE) for i in range(N_PARALLEL_GAMES)]\n",
    "for atari in ataries:\n",
    "    atari.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interact(ataries,n_steps = 100,verbose=False):\n",
    "    \"\"\"generate interaction sessions with ataries (openAI gym atari environments)\n",
    "    Sessions will have length n_steps. \n",
    "    Each time one of games is finished, it is immediately getting reset\"\"\"\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = map(lambda atari: atari.render('rgb_array'), ataries)\n",
    "    \n",
    "    prev_memory_states = 'zeros'\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        actions,new_memory_states = step(prev_observations,prev_memory_states)\n",
    "\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = zip(*map(lambda atari, action: atari.step(action), \n",
    "                                                            ataries,actions))\n",
    "        \n",
    "        for i in range(len(ataries)):\n",
    "            if is_done[i]:\n",
    "                ataries[i].reset()\n",
    "                if verbose:\n",
    "                    print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,new_memory_states,is_done,infos))\n",
    "        \n",
    "        prev_observations = new_observations\n",
    "        prev_memory_states = new_memory_states\n",
    "                \n",
    "    \n",
    "    \n",
    "    return zip(*history_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "obsevation_log,action_log,reward_log,_,_,_  = interact(ataries,50)\n",
    "\n",
    "\n",
    "print np.array(reward_log)[:10].T\n",
    "print np.array(action_names)[np.array(action_log)[:3,:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "### we shall use session pool environment to immediately train on new sessions\n",
    "* theoretically, this environemnt is designed for storing a lot of game sessions and training on random batches,\n",
    "* but for the sake of baseline, it's a one-time usage pool\n",
    "\n",
    "1. Interact with ALE, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_pool(env, ataries,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    \n",
    "    obsevation_log,action_log,reward_log,_,is_done_log,_= interact(ataries,n_steps=n_steps)\n",
    "    \n",
    "    \n",
    "    #tensor dimensions\n",
    "    \n",
    "    # [batch_i, time_i, width, height, rgb]\n",
    "    observation_tensor = np.array(obsevation_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i,time_i]\n",
    "    action_tensor = np.array(action_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i, time_i]\n",
    "    reward_tensor = np.array(reward_log).swapaxes(0,1)\n",
    "\n",
    "    # [batch_i, time_i]\n",
    "    is_alive_tensor = 1- np.array(is_done_log,dtype = 'int8').swapaxes(0,1)\n",
    "    \n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,[])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,ataries,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#produce interaction sequences of length <= 10\n",
    "\n",
    "_,observation_seq,_,_,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "#observation seq are the observation tensor we just loaded\n",
    "#qvalues seq are agent's Qvalues obtained via experience replay\n",
    "\n",
    "\n",
    "#The three \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#second - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n",
    "\n",
    "#the actions agent took in the original recorded game\n",
    "action_seq = env.actions[0]\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.rewards\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.is_alive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.95),name = 'q_learning_gamma')\n",
    "\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                        action_seq,\n",
    "                                                        rewards_seq,\n",
    "                                                        is_alive_seq,\n",
    "                                                        gamma_or_gammas=gamma,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "mse_loss = elwise_mse_loss.sum() / is_alive_seq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = lasagne.updates.adadelta(loss,\n",
    "                                             weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,mse_loss,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session visualization tools\n",
    "\n",
    "### [warning, this thing basicly tries to track various Qvalues over time]\n",
    "### [but it's bulky and stupid, so don't try to understand it if it didn't come naturally]\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* usually useful to get insight on what worked and what din't\n",
    "* in this case, we print strings following pattern\n",
    "  * [action_name] ([predicted action qvalue]) -> reward [reference qvalue] | next iteration\n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are Qvalues for actions\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "__Warning! the visualization tools are underdeveloped and only allow simple operations.__\n",
    "\n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        qvalues_seq, action_seq,rewards_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False,max_n_sessions = 3,update = True):\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in ataries[:max_n_sessions]]\n",
    "    \n",
    "    if update:\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "    \n",
    "    \n",
    "    printables = get_printables()\n",
    "    \n",
    "    \n",
    "    for i in range(max_n_sessions):\n",
    "        plt.imshow(pictures[i])\n",
    "        plt.show()\n",
    "            \n",
    "        qvalues_log,actions_log,reward_log, is_alive_log = map(lambda v: np.array(v[i:i+1]), printables)\n",
    "        \n",
    "\n",
    "        print_sessions(qvalues_log,actions_log,reward_log,\n",
    "                       is_alive_seq = is_alive_log,\n",
    "                       action_names=action_names,\n",
    "                       legend = True, #do not show legend since there's too many labeled objects\n",
    "                      plot_policy = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "epsilon.set_value(0.05)\n",
    "display_sessions(with_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tools for model persistence (in progress now. Requires unique names)\n",
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 25000\n",
    "batch_size= 10\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,ataries,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/500.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%50 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "\n",
    "        print \"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy)\n",
    "        print \"rec %.3f reg %.3f\"%(q_loss,l2_penalty)\n",
    "\n",
    "    if epoch_counter %1000 ==0:\n",
    "        print \"Learning curves:\"\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print \"Random session examples\"\n",
    "        display_sessions(with_plots=False)\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %5000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print \"saved\", snap_name\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random session examples\"\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(ataries,n_steps = 100):\n",
    "\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = map(lambda atari: atari.render('rgb_array'), ataries)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        #show pics\n",
    "        map(lambda atari: atari.render('human'), ataries)\n",
    "\n",
    "        \n",
    "        actions = applier_fun(prev_observations)[0]\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = zip(*map(lambda atari, action: atari.step(action), \n",
    "                                                            ataries,actions))\n",
    "        \n",
    "        for i in range(len(ataries)):\n",
    "            if is_done[i]:\n",
    "                ataries[i].reset()\n",
    "                print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,is_done))\n",
    "        prev_observations = new_observations\n",
    "                \n",
    "    \n",
    "    \n",
    "    return zip(*history_log)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "show(ataries[:1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
